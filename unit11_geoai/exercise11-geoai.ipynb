{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoAI\n",
    "## Prediction of Population Distribution with Random Forest Regressor\n",
    "\n",
    "Required datasets (download from LearnIT): \n",
    "    \n",
    "- raster_data : A folder including various raster layers about the study area\n",
    "(GHS: population distrubution, mean building construction year and number of floors, housing prices of sold properties in 2018, mean prices of sold properties in the last decade, proximity to bus stations, railway stations, cultural spaces, schools, Corine land cover)\n",
    "\n",
    "Take a look at the data and let's discuss!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import os, pandas as pd, numpy as np\n",
    "import rasterio as rs\n",
    "# initial directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function to read a raster layer\n",
    "def readRaster(file):\n",
    "    src = rs.open(file)\n",
    "    arr = src.read(1)\n",
    "    arr = np.nan_to_num(arr, nan=0, posinf=0, neginf=0) \n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What are we doing here? \n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"y\">\n",
    "<b>Optional hint for <code><font size=\"4\"> changing dimensions</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1. You'll find [flatten()](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.flatten.html) useful!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Steps:\n",
    "        #       1) Provide the directory to raster_data.\n",
    "        #       2) Change the dimensionality of the input layers\n",
    "dataPath = \"\"\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in os.listdir(dataPath):\n",
    "    if i.endswith(\".tif\"):\n",
    "        name = i.split(\".tif\")[0]\n",
    "        arr = readRaster(dataPath + i)\n",
    "        df[\"{}\".format(name)] = arr\n",
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot one of the input layers (e.g., GHS). First, plot its histogram, define the color scale and the bins, and finally plot and save the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Steps:\n",
    "        #       1) Provide the directory to raster_data.\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show_hist\n",
    "plt.figure(figsize=(20, 10))\n",
    "src = rs.open()\n",
    "show_hist(src, bins=20, lw=0.0, stacked=False, alpha=0.5 ,title=\"Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# This function defines the bins for population distribution\n",
    "def defineBins(valMin, valMax):\n",
    "    valMax = int(round(valMax,1))\n",
    "    valMin = int(round(valMin,1))\n",
    "    bins = 7\n",
    "    a=[valMin, 100, 200, 300, 400, 500, 600, valMax]\n",
    "    colors_palette =[\"#f1eef6\",\"#e0c8e2\",\"#da9acb\", \"#df65b0\", \"#de348a\", \"#c61266\",\"#980043\"]\n",
    "    cmap = ListedColormap(colors_palette )\n",
    "    norm = colors.BoundaryNorm(a, bins)  \n",
    "    # Add a legend for labels\n",
    "    legend_labels = { colors_palette[0]: \"<{1}\".format(a[0], a[1]), colors_palette[1]: \"{0}-{1}\".format(a[1], a[2]), \n",
    "        colors_palette[2]: \"{0}-{1}\".format(a[2],a[3]), colors_palette[3]: \"{0}-{1}\".format(a[3],a[4]),\n",
    "        colors_palette[4]: \"{0}-{1}\".format(a[4],a[5]), colors_palette[5]: \"{0}-{1}\".format(a[5],a[6]), \n",
    "        colors_palette[6]: \">{0}\".format(a[6],a[7])}\n",
    "    \n",
    "    return cmap, norm, legend_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the GHS layer and save it as an image\n",
    "#     Steps:\n",
    "        #       1) Provide the directory to Greater Copenhagen vector layer.\n",
    "        #       2) Provide the directory to raster_data.\n",
    "        #       3) Provide a map title.\n",
    "        #       4) Provide a directory to save the image.\n",
    "import geopandas as gpd\n",
    "import rasterio.plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "fig, ax = plt.subplots(figsize=(20, 20),facecolor='white') #50, 50\n",
    "\n",
    "districtPath = \"\"\n",
    "srcfile = gpd.read_file(districtPath, encoding =\"latin-1\" )\n",
    "srcfile.plot(ax=ax, facecolor='none', edgecolor='#000000', linewidth=0.8,  zorder=2 )\n",
    "src = rs.open()\n",
    "arr = src.read(1)\n",
    "arr = np.nan_to_num(arr, nan=0, posinf=0, neginf=0) \n",
    "valMax = np.round(np.max(arr), 2)\n",
    "valMin = np.round(np.min(arr), 2)\n",
    "#Define the color ramp and the legend\n",
    "cmap, norm, legend_labels = defineBins(valMin, valMax)\n",
    "\n",
    "waterTif = rasterio.open(cwd + \"/data/raster_data/waterComb_cph_CLC_2012_2018.tif\")\n",
    "# colors for water layer\n",
    "cmapWater = ListedColormap([\"#00000000\",\"#7fa9b060\" ])\n",
    "rasterio.plot.show(waterTif, ax=ax, cmap=cmapWater, zorder=5)\n",
    "rasterio.plot.show(src, ax=ax, cmap=cmap, norm=norm, extent= [src.bounds[0],src.bounds[1], src.bounds[2], src.bounds[3]], zorder=1)  \n",
    "srcfile['coords']= srcfile['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "srcfile['coords'] = [coords[0] for coords in srcfile['coords']]\n",
    "for idx, row in srcfile.iterrows():\n",
    "    plt.annotate(text = row['KOMNAVN'], xy=row['coords'], horizontalalignment= 'center', fontsize=12, zorder=20)\n",
    "\n",
    "# Add a title\n",
    "title = \"\"\n",
    "ax.set_title(title, color=\"black\", fontsize=18)\n",
    "# Connect labels and colors, create legend and save the image\n",
    "patches = [mpatches.Patch(color=color, label=label)\n",
    "            for color, label in legend_labels.items()]     \n",
    "       \n",
    "ax.legend(handles=patches,  loc='lower right', facecolor=\"white\", fontsize=12, title = 'Population', title_fontsize=14).set_zorder(6)\n",
    "print(\"-----PLOTTING IMAGE {}-----\".format(title))\n",
    "ax.axes.xaxis.set_visible(False)\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "#plt.savefig(, dpi=300, bbox_inches='tight', facecolor=fig.get_facecolor(),transparent=True)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the color scale, move the legend to top right corner of the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Targets\n",
    "We need to separate the data into features and targets. The target, or label, is the value we want to predict, the population distribution. The features are the training data, or all the columns the model uses to make a prediction. We need to convert the Pandas dataframes to Numpy arrays because that is the way the algorithm works. How else could we have done it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "\n",
    "#     Steps:\n",
    "        #       1) Select the target column.\n",
    "        #       2) Remove it from the training data.\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(features[])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = features.drop(, axis = 1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Sets\n",
    "We are splitting the data into training and testing sets (75% of the dataset to train it and 25% to test). During the training, we let the model 'see' the answers, the population distribution, so it can learn how to predict it from the features. We suspect relationships between the features and the target values (higher population close to central points, no population in the sea), and the model’s job is to learn this relationship and the patterns during training. Then, when we evaluate the model, we make predictions on the testing set where it only has access to the features (not the answers)! As we have the actual answers for the test set, we can directly compare these predictions to the true value to judge how accurate the model is. Generally, when we train a model, we need to randomly split the data into training and testing sets to get a representation of all data points. However, it could also be trained in the whole dataset. I am setting the random state to 42 which means the results will be the same each time I run the split for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#     Steps:\n",
    "        #       1) Select the percentage for splitting the dataset to training and testing set.\n",
    "        \n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "After all the work of data preparation, creating and training, the model is very simple using Scikit-learn. We import the random forest regression model from skicit-learn, instantiate the model, and fit (scikit-learn’s name for training) the model on the training data. We set the random state for reproducible results. This entire process is only 3 lines in scikit-learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Steps:\n",
    "        #       1) Select the number of trees and the depth of model. Recommended: 100, 6\n",
    "        #       2) Select the training features and the target set.\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 100 decision trees and depth of 6\n",
    "rf = RandomForestRegressor(n_estimators = , max_depth = , random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions on the Training Set\n",
    "Our model has now learnt the relationships between the features and the targets. The next step is to evaluate how good the model is! We make predictions on the test features and the model is not allowed to see the test answers. We then compare the predictions to the known answers. When performing regression, we need to make sure to use the absolute error because we expect some of our answers to be low and some to be high. We are interested to see how far away our average prediction is from the actual value so we take the absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Steps:\n",
    "        #       1) Make a prediction on the testing set \n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict()\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 1), 'persons.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What other evaluation metrics can we use? What do we need to know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Model and Report Results\n",
    "At this point, we se that our model is not very good, but letøs try to understand how it works and why it fails. The question is: how does this model arrive at the values? There are two approaches to get under the hood of the random forest: 1. we can look at a single tree in the forest, 1. we can look at the feature importances of our explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools needed for visualization (you can skip thi step if you do not have pydot installed)\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Limit depth of tree to 3 levels\n",
    "rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\n",
    "rf_small.fit(train_features, train_labels)\n",
    "# Extract the small tree\n",
    "tree_small = rf_small.estimators_[5]\n",
    "# Save the tree as a png image\n",
    "export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "graph.write_png('data/small_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrate the relative significance of the variables (bar chart). \n",
    "\n",
    "Try to change the capacity of the model and its depth. \n",
    "How do the errors and the variable importances change? What if you remove the variables with the lowest importance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_feature_importance(importances,file):\n",
    "    \n",
    "    x_values = list(range(len(importances)))\n",
    "   \n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x_values, importances, orientation = 'vertical')\n",
    "    plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "    #Add chart labels\n",
    "    plt.title('Importance')\n",
    "    plt.ylabel('FEATURE IMPORTANCE')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.show()\n",
    "    #plt.savefig(file, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Steps:\n",
    "        #       1) Plot the feature importances \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Visualize the Prediction\n",
    "Let's use the whole dataset to make predictions and visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Steps:\n",
    "        #       1) Make a prediction on the whole dataset\n",
    "        #       2) Calculate the MAE\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict()\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 1), 'persons.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Steps:\n",
    "        #       1) Open the original GHS file\n",
    "        #       2) Reshape the predictions to the GHS shape\n",
    "        #       3) Give a destination file for the predictions\n",
    "original_img = rs.open()\n",
    "original_arr = original_img.read(1)\n",
    "# Extract spatial metadata\n",
    "input_crs = original_img.crs\n",
    "input_gt  = original_img.transform\n",
    "\n",
    "# Reshape the predictions\n",
    "predictions = predictions.reshape(original_arr.shape[0],original_arr.shape[1] )\n",
    "\n",
    "# Prepare output geotiff file. We give crs and gt read from input as spatial metadata\n",
    "with rs.open(\n",
    "  ,\n",
    "  'w',\n",
    "  driver = 'GTiff',\n",
    "  count = 1,\n",
    "  height = original_arr.shape[0],\n",
    "  width  = original_arr.shape[1],\n",
    "  dtype  = original_arr.dtype,\n",
    "  crs    = input_crs,\n",
    "  transform = input_gt  \n",
    ") as output:\n",
    "  output.write(predictions, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio.plot\n",
    "#Read the predictions file\n",
    "src = rs.open(cwd + \"/prediction.tif\")\n",
    "pop = src.read(1)\n",
    "\n",
    "valMax = np.round(np.max(pop), 2)\n",
    "valMin = np.round(np.min(pop), 2)\n",
    "print(valMin,valMax)\n",
    "fig, ax = plt.subplots(figsize=(10, 10),facecolor='white') #50, 50\n",
    "#Define the color ramp and the legend\n",
    "cmap, norm, legend_labels = defineBins(valMin, valMax)\n",
    "\n",
    "#Add municipality borders and names\n",
    "kommune = gpd.read_file(cwd + \"/data/GreaterCopenhagen.gpkg\")\n",
    "kommune.plot(ax=ax, facecolor='none', edgecolor='#000000', linewidth=.3, alpha=0.6, zorder=3 )\n",
    "kommune['coords']= kommune['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "kommune['coords'] = [coords[0] for coords in kommune['coords']]\n",
    "for idx, row in kommune.iterrows():\n",
    "    plt.annotate(text = row['KOMNAVN'], xy=row['coords'], horizontalalignment= 'center', fontsize=8) \n",
    "xlim = ([kommune.total_bounds[0],  kommune.total_bounds[2]])\n",
    "ylim = ([kommune.total_bounds[1],  kommune.total_bounds[3]])\n",
    "\n",
    "#Add water\n",
    "waterTif = rs.open(cwd + \"/data/raster_data/waterComb_cph_CLC_2012_2018.tif\")\n",
    "# colors for water layer\n",
    "cmapWater = ListedColormap([\"#00000000\",\"#7fa9b060\" ])\n",
    "rasterio.plot.show(waterTif, ax=ax, cmap=cmapWater, zorder=2)           \n",
    "rasterio.plot.show(src, ax=ax, cmap=cmap, norm=norm, extent= [src.bounds[0],src.bounds[1], src.bounds[2], src.bounds[3]], zorder=1) \n",
    "\n",
    "patches = [mpatches.Patch(color=color, label=label)\n",
    "                for color, label in legend_labels.items()] \n",
    "#Add legend\n",
    "plt.legend(handles=patches,  loc='upper right', facecolor=\"white\", fontsize=8, title = \"Population Distribution\", title_fontsize=8).set_zorder(4)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Prediction\")\n",
    "plt.savefig(cwd + \"/prediction.png\", dpi=300, bbox_inches='tight', facecolor=fig.get_facecolor(),transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Advanced) Compare your prediction to the original data on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c726fd91347bc208880e08109d6695b53d962b42159a79691673c8f5d7db421b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
